{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of hill climbing the test set for a classification task\n",
    "from random import randint\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load or prepare the classification dataset\n",
    "def load_dataset():\n",
    "    return make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "\n",
    "# evaluate a set of predictions\n",
    "def evaluate_predictions(y_test, yhat):\n",
    "    return accuracy_score(y_test, yhat)\n",
    "\n",
    "# create a random set of predictions\n",
    "def random_predictions(n_examples):\n",
    "    return [randint(0, 1) for _ in range(n_examples)]\n",
    "\n",
    "# modify the current set of predictions\n",
    "def modify_predictions(current, n_changes=1):\n",
    "    # copy current solution\n",
    "    updated = current.copy()\n",
    "    for i in range(n_changes):\n",
    "        # select a point to change\n",
    "        ix = randint(0, len(updated)-1)\n",
    "        # flip the class label\n",
    "        updated[ix] = 1 - updated[ix]\n",
    "    return updated\n",
    "\n",
    "# run a hill climb for a set of predictions\n",
    "def hill_climb_testset(X_test, y_test, max_iterations):\n",
    "    scores = list()\n",
    "    # generate the initial solution\n",
    "    solution = random_predictions(X_test.shape[0])\n",
    "    # evaluate the initial solution\n",
    "    score = evaluate_predictions(y_test, solution)\n",
    "    scores.append(score)\n",
    "    # hill climb to a solution\n",
    "    for i in range(max_iterations):\n",
    "        # record scores\n",
    "        scores.append(score)\n",
    "        # stop once we achieve the best score\n",
    "        if score == 1.0:\n",
    "            break\n",
    "        # generate new candidate\n",
    "        candidate = modify_predictions(solution)\n",
    "        # evaluate candidate\n",
    "        value = evaluate_predictions(y_test, candidate)\n",
    "        # check if it is as good or better\n",
    "        if value >= score:\n",
    "            solution, score = candidate, value\n",
    "            print('>%d, score=%.3f' % (i, score))\n",
    "    return solution, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "X, y = load_dataset()\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# run hill climb\n",
    "yhat, scores = hill_climb_testset(X_test, y_test, 20000)\n",
    "\n",
    "# plot the scores vs iterations\n",
    "pyplot.plot(scores)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
