{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.programmersought.com/article/79252859610/\n",
    "import matplotlib.pyplot as mp, numpy as np\n",
    "\n",
    "# Primitive\n",
    "origin = lambda x: 2 * x - x ** 2\n",
    "x = np.linspace(0, 2, 9999)\n",
    "mp.plot(x, origin(x), c='black')  # Visualization\n",
    "\n",
    "# Derivative of the original function\n",
    "derivative = lambda x: 2 - 2 * x\n",
    "\n",
    "# Gradient rising demand\n",
    "extreme_point = 0  # Initial value\n",
    "alpha = 0.1  # Step, that is the learning rate\n",
    "precision = 0.001  # Range of tolerance\n",
    "\n",
    "while True:\n",
    "    mp.scatter(extreme_point, origin(extreme_point))  # Visualization\n",
    "    error = alpha * derivative(extreme_point)  # Climbing pace\n",
    "    extreme_point += error  #Climbing #\n",
    "    if abs(error) < precision:\n",
    "        break  # Exit iterative error is small\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X, y = make_classification(1000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, flip_y=0.05, random_state=4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "np.random.seed(4)\n",
    "W = np.random.randn(2)\n",
    "\n",
    "def predict(x, w):\n",
    "    return 1 / (1 + np.exp(-np.matmul(x, w)))\n",
    "\n",
    "def estimate_gradient(x, y, w):\n",
    "    return np.nan_to_num((2 * y - 1) / predict(x, W) * x * np.nan_to_num(np.exp(-np.matmul(x, w))) / (np.nan_to_num(np.exp(-np.matmul(x, w))) + 1)**2)\n",
    "\n",
    "a = 0.005 #learning rate\n",
    "num_epochs = 45 #numbver of iteration\n",
    "\n",
    "predictions = predict(X_train, W)\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "parameter_values = np.reshape(W, (1, -1))\n",
    "accuracy_ratios = [accuracy_score(y_train, predictions)]\n",
    "for i in range(num_epochs):\n",
    "    for j in range(X_train.shape[0]):\n",
    "        W = W + a * estimate_gradient(X_train[j], y_train[j], W)\n",
    "    parameter_values = np.concatenate((parameter_values, np.reshape(W, (1, -1))), axis=0)\n",
    "    predictions = predict(X_train, W)\n",
    "    predictions = predictions.astype(int)\n",
    "    accuracy_ratios.append(accuracy_score(y_train, predictions))\n",
    "\n",
    "#training\n",
    "predictions = predict(X_train, W)\n",
    "predictions = predictions.astype(int)\n",
    "print(\"Training: \", accuracy_score(y_train, predictions))\n",
    "\n",
    "#testing\n",
    "predictions = predict(X_test, W)\n",
    "predictions = predictions.astype(int)\n",
    "print(\"Testing: \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from pandas import DataFrame\n",
    "import time as time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import csv\n",
    "\n",
    "def get_data(filename):  \n",
    "    with open(filename, 'r') as f:\n",
    "        data = list(csv.reader(f, delimiter=\",\"))\n",
    "\n",
    "    data = np.array(data, dtype=np.float)\n",
    "    df = DataFrame(data, columns=['x0','x1','x2','label'])\n",
    "\n",
    "    dataMat = df.loc[:,['x0','x1','x2']]\n",
    "    labelMat = df.loc[:,['label']]\n",
    "    return dataMat,labelMat\n",
    "    \n",
    "def gradAscent(dataMat,labelMat,alpha = 0.1,maxstep = 1000): #Batch gradient ascent\n",
    "    start_time = time.time()\n",
    "    m,n = dataMat.shape\n",
    "    weights = np.ones((n,1))\n",
    "    labelMat = labelMat.values.reshape((16,1)) \n",
    "    for i in range(maxstep):\n",
    "        h = sigmoid(np.dot(dataMat,weights).astype('int64'))  #Here is the matrix operation directly\n",
    "        #labelMat = labelMat.values.reshape((30,1))                  #label is a one-dimensional, converted to two-dimensional\n",
    "        error = labelMat-h                                    #Batch calculation error\n",
    "        weights = weights + alpha*np.dot(dataMat.T,error)     #Update weight\n",
    "        \n",
    "    print(\"x1*\", weights[0], \" + x2*\", weights[1], \" + x3*\", weights[2])\n",
    "    duration = time.time()-start_time\n",
    "    print('time:',duration)\n",
    "    return weights\n",
    "\n",
    "def stocGradAscent(dataMat,labelMat,alpha = 0.01):   #Random gradient ascent\n",
    "    start_time = time.time()                         #Record program start time\n",
    "    m,n = dataMat.shape\n",
    "    weights = np.ones((n,1))                         #Allocation weight is 1\n",
    "    \n",
    "    for i in range(m):\n",
    "        h = sigmoid(np.dot(dataMat.iloc[i],weights).astype('int64')) #Note: The dtype obtained from the inner product of the two two-dimensional arrays here is object, which needs to be converted to int64\n",
    "        error = labelMat.iloc[i]-h                        #error\n",
    "        weights = weights + alpha*dataMat.iloc[i].values.reshape((3,1))*error[0] #Update weight\n",
    "    duration = time.time()-start_time\n",
    "    print('time:',duration)\n",
    "    return weights\n",
    "\n",
    "def betterStoGradAscent(dataMat,labelMat,alpha = 0.01,maxstep = 150):\n",
    "    start_time = time.time()\n",
    "    m,n = dataMat.shape\n",
    "    weights = np.ones((n,1))\n",
    "    for j in range(maxstep):\n",
    "        for i in range(m):\n",
    "            alpha = 4/(1+i+j) + 0.01                         #Set the update rate to decrease with iteration\n",
    "            h = sigmoid(np.dot(dataMat.iloc[i],weights).astype('int64'))\n",
    "            error = labelMat.iloc[i]-h\n",
    "            weights = weights + alpha*dataMat.iloc[i].values.reshape((3,1))*error[0]\n",
    "    duration = time.time()-start_time\n",
    "    print('time:',duration)\n",
    "    return weights\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def show(dataMat, labelMat, weights):\n",
    "    #dataMat = np.mat(dataMat)\n",
    "    #labelMat = np.mat(labelMat)\n",
    "    m,n = dataMat.shape\n",
    "    min_x = min(dataMat.iloc[:, 1])\n",
    "    max_x = max(dataMat.iloc[:, 1])\n",
    "    xcoord1 = []; ycoord1 = []\n",
    "    xcoord2 = []; ycoord2 = []\n",
    "    for i in range(m):\n",
    "        if int(labelMat.iloc[i]) == 0:            \n",
    "            xcoord1.append(dataMat.iloc[i, 1]); ycoord1.append(dataMat.iloc[i, 2])\n",
    "        elif int(labelMat.iloc[i]) == 1:\n",
    "            xcoord2.append(dataMat.iloc[i, 1]); ycoord2.append(dataMat.iloc[i, 2])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(xcoord1, ycoord1, s=30, c=\"red\", marker=\"s\")\n",
    "    ax.scatter(xcoord2, ycoord2, s=30, c=\"green\")\n",
    "    x = np.arange(min_x, max_x, 0.1)\n",
    "    y = (-float(weights[0]) - float(weights[1])*x) / float(weights[2])\n",
    "    ax.plot(x, y)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    dataMat,labelMat = get_data('data2.csv')\n",
    "    #weights = gradAscent(dataMat,labelMat)\n",
    "    #weights = stocGradAscent(dataMat,labelMat)\n",
    "    weights = betterStoGradAscent(dataMat,labelMat)\n",
    "    show(dataMat,labelMat,weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#read the data\n",
    "data = pd.read_csv(\"divorce.csv\", delimiter = ';')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "value = [1]*170\n",
    "#inserting a bias \n",
    "df = df.insert(0,\"Atr0\",value,False)\n",
    "\n",
    "x = data.iloc[: , :-1]\n",
    "y = data.iloc[: , -1]\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "#Split the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.15, random_state=0) \n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        '''Initialize variables\n",
    "        Args:\n",
    "            learning_rate  : Learning Rate\n",
    "            max_iterations : Max iterations for training weights\n",
    "        '''\n",
    "        # Initialising all the parameters\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.likelihoods    = []\n",
    "        \n",
    "        # Define epsilon because log(0) is not defined\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        '''Sigmoid function: f:R->(0,1)\n",
    "        Args:\n",
    "            z : A numpy array (num_samples,)\n",
    "        Returns:\n",
    "            A numpy array where sigmoid function applied to every element\n",
    "        '''\n",
    "        ### START CODE HERE\n",
    "        sig_z = (1/(1+np.exp(-z)))\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'\n",
    "        return sig_z\n",
    "    \n",
    "    def log_likelihood(self, y_true, y_pred):\n",
    "        '''Calculates maximum likelihood estimate\n",
    "        Remember: y * log(yh) + (1-y) * log(1-yh)\n",
    "        Note: Likelihood is defined for multiple classes as well, but for this dataset\n",
    "        we only need to worry about binary/bernoulli likelihood function\n",
    "        Args:\n",
    "            y_true : Numpy array of actual truth values (num_samples,)\n",
    "            y_pred : Numpy array of predicted values (num_samples,)\n",
    "        Returns:\n",
    "            Log-likelihood, scalar value\n",
    "        '''\n",
    "        # Fix 0/1 values in y_pred so that log is not undefined\n",
    "        y_pred = np.maximum(np.full(y_pred.shape, self.eps), np.minimum(np.full(y_pred.shape, 1-self.eps), y_pred))\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        likelihood = (y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n",
    "    \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return np.mean(likelihood)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Trains logistic regression model using gradient ascent\n",
    "        to gain maximum likelihood on the training data\n",
    "        Args:\n",
    "            X : Numpy array (num_examples, num_features)\n",
    "            y : Numpy array (num_examples, )\n",
    "        Returns: VOID\n",
    "        '''\n",
    "        \n",
    "        num_examples = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        \n",
    "        # Initialize weights with appropriate shape\n",
    "        self.weights = np.zeros((X.shape[1]))\n",
    "        # print(\"Z\",self.weights.shape)\n",
    "        # print(X.shape)\n",
    "        \n",
    "        \n",
    "        # Perform gradient ascent\n",
    "        for i in range(self.max_iterations):\n",
    "            # Define the linear hypothesis(z) first\n",
    "            # HINT: what is our hypothesis function in linear regression, remember?\n",
    "            \n",
    "            z  = np.dot(X,self.weights)\n",
    "          \n",
    "            # Output probability value by appplying sigmoid on z\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Calculate the gradient values\n",
    "            # This is just vectorized efficient way of implementing gradient. \n",
    "            gradient = np.mean((y-y_pred)*X.T, axis=1)\n",
    "            \n",
    "            # Update the weights\n",
    "            # Caution: It is gradient ASCENT not descent\n",
    "            self.weights +=  self.learning_rate*gradient\n",
    "            \n",
    "            # Calculating log likelihood\n",
    "            likelihood = self.log_likelihood(y,y_pred)\n",
    "\n",
    "            self.likelihoods.append(likelihood)\n",
    "    \n",
    "        ### END CODE HERE\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        '''Predict probabilities for given X.\n",
    "        Remember sigmoid returns value between 0 and 1.\n",
    "        Args:\n",
    "            X : Numpy array (num_samples, num_features)\n",
    "        Returns:\n",
    "            probabilities: Numpy array (num_samples,)\n",
    "        '''\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Fit the model before prediction\")\n",
    "      \n",
    "        ### START CODE HERE\n",
    "               \n",
    "        z = np.dot(X,self.weights)\n",
    "        probabilities = self.sigmoid(z)\n",
    "        # probabilities.reshape(probabilities.shape[0],1)\n",
    "        \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        '''Predict/Classify X in classes\n",
    "        Args:\n",
    "            X         : Numpy array (num_samples, num_features)\n",
    "            threshold : scalar value above which prediction is 1 else 0\n",
    "        Returns:\n",
    "            binary_predictions : Numpy array (num_samples,)\n",
    "        '''\n",
    "        # Thresholding probability to predict binary values\n",
    "        \n",
    "        binary_predictions = np.array(list(map(lambda x: 1 if x>threshold else 0, self.predict_proba(X))))\n",
    "        \n",
    "        return binary_predictions\n",
    "\n",
    "#create learning model\n",
    "model = MyLogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    '''Compute accuracy.\n",
    "    Accuracy = (Correct prediction / number of samples)\n",
    "    Args:\n",
    "        y_true : Truth binary values (num_examples, )\n",
    "        y_pred : Predicted binary values (num_examples, )\n",
    "    Returns:\n",
    "        accuracy: scalar value\n",
    "    '''\n",
    "    count=0\n",
    "    ### START CODE HERE\n",
    "    y_cap = y_pred - y_true\n",
    "    count = np.count_nonzero(y_cap==0)   \n",
    "    accuracy = (count/len(y_pred))*100\n",
    "    ### END CODE HERE\n",
    "    return accuracy\n",
    "\n",
    "#get the accuracy\n",
    "accuracy(y_test,y_pred)\n",
    "\n",
    "plt.plot([i+1 for i in range(len(model.likelihoods))], model.likelihoods)\n",
    "plt.title(\"Log-Likelihood curve\")\n",
    "plt.xlabel(\"Iteration num\")\n",
    "plt.ylabel(\"Log-likelihood\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
